---
title: "Preprocessing and resampling using #TidyTuesday college data"
date: 2020-03-10
slug: "tuition-resampling"
tags: [rstats,tidymodels]
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, dpi = 300, cache.lazy = FALSE,
                      tidy = "styler", fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(silgelib)
theme_set(theme_plex())
```

I've been publishing [screencasts](https://juliasilge.com/tags/tidymodels/) demonstrating how to use the tidymodels framework, from first getting started to how to tune machine learning models. Today, I'm using this week's [`#TidyTuesday` dataset](https://github.com/rfordatascience/tidytuesday) on college tuition and diversity at US colleges to show some data preprocessing steps and how to use resampling!

```{r, echo=FALSE}
blogdown::shortcode("youtube", "s3TkvZM60iU")
```

</br>

Here is the code I used in the video, for those who prefer reading instead of or in addition to video.

## Explore the data

Our modeling goal here is to predict which US colleges have higher proportions of minority students based on college data such as tuition from the [#TidyTuesday dataset](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-10/readme.md). There are several related datasets this week, and this modeling analysis uses two of them.

```{r}
library(tidyverse)

tuition_cost <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/tuition_cost.csv')

diversity_raw <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-10/diversity_school.csv') %>% 
  filter(category == "Total Minority") %>%
  mutate(TotalMinority = enrollment / total_enrollment)
```

What is the distribution of total minority student population?

```{r}
diversity_school <- diversity_raw %>% 
  filter(category == "Total Minority") %>%
  mutate(TotalMinority = enrollment / total_enrollment)

diversity_school %>%
  ggplot(aes(TotalMinority)) +
  geom_histogram(alpha = 0.7, fill = "midnightblue") + 
  scale_x_continuous(labels = scales::percent_format()) + 
  labs(x = "% of student population who identifies as any minority")
```

The median proportion of minority students for this dataset is `r percent(median(diversity_school$TotalMinority))`.

Let's build a dataset for modeling, joining the two dataframes we have. Let's also move from individual states in the US to US regions, as found in `state.region`.

```{r render = knitr::normal_print}
university_df <- diversity_school %>% 
  filter(category == "Total Minority") %>%
  mutate(TotalMinority = enrollment / total_enrollment) %>%
  transmute(diversity = case_when(TotalMinority > 0.3 ~ "high",
                                  TRUE ~ "low"),
            name, state,
            total_enrollment) %>%
  inner_join(tuition_cost %>%
               select(name, type, degree_length, 
                      in_state_tuition:out_of_state_total)) %>%
  left_join(tibble(state = state.name, region = state.region)) %>%
  select(-state, -name) %>%
  mutate_if(is.character, factor)

skimr::skim(university_df)
```

How are some of these quantities related to the proportion of minority students at a college?

```{r}
university_df %>%
  ggplot(aes(type, in_state_tuition, fill = diversity)) +
  geom_boxplot(alpha = 0.8) +
  scale_y_continuous(labels = scales::dollar_format()) + 
  labs(x = NULL, y = "In-State Tuition", fill = "Diversity")
```

## Build models with recipes

Now it is time for modeling! First, we split our data into training and testing sets. Then, we build a recipe for data preprocessing.

- First, we must tell the `recipe()` what our model is going to be (using a formula here) and what our training data is.
- We then filter out variables that are too correlated with each other. We had several different ways of measuring the tuition in our dataset that are correlated with each other, and this step shows how to handle a situation like that.
- We then convert the factor columns into (one or more) numeric binary (0 and 1) variables for the levels of the training data.
- Next, we remove any numeric variables that have zero variance.
- As a last step, we normalize (center and scale) the numeric variables. We need to do this because some of them are on very different scales from each other and a model we want to train is sensitive to this.
- Finally, we `prep()` the `recipe()`. This means we actually do something with the steps and our training data; we estimate the required parameters from `uni_train` to implement these steps so this whole sequence can be applied later to another dataset.


```{r}
library(tidymodels)

set.seed(1234)
uni_split <- initial_split(university_df, strata = diversity)
uni_train <- training(uni_split)
uni_test <- testing(uni_split)

uni_rec <- recipe(diversity ~ ., data = uni_train) %>%
  step_corr(all_numeric()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_numeric())

uni_prep <- uni_rec %>%
  prep()

uni_prep
```

Now it's time to **specify** and then **fit** our models. Here, we specify and fit three models: 

- logistic regression
- k-nearest neighbor
- decision tree

Check out what data we are training these models on: `juice(uni_rec)`. The recipe `uni_rec` contains all our transformations for data preprocessing and feature engineering, *as well as* the data these transformations were estimated from. When we `juice()` the recipe, we squeeze that training data back out, transformed in all the ways we specified.

```{r}
uni_juiced <- juice(uni_prep)

glm_spec <- logistic_reg() %>%
  set_engine("glm")

glm_fit <- glm_spec %>%
  fit(diversity ~ ., data = uni_juiced)

glm_fit

knn_spec <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_fit <- knn_spec %>%
  fit(diversity ~ ., data = uni_juiced)

knn_fit

tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_fit <- tree_spec %>%
  fit(diversity ~ ., data = uni_juiced)

tree_fit
```

Models! `r emo::ji("tada")`

## Evaluate models with resampling

Well, we fit models, but how do we evaluate them? We can use resampling to compute performance metrics across some set of resamples, like the cross-validation splits we create here. The function `fit_resamples()` fits a model such as `glm_spec` to the analysis subset of each resample and evaluates on the heldout bit (the assessment subset) from each resample. We can use `metrics = metric_set()` to specify which metrics we want to compute if we don't want to only use the default ones; here let's check out sensitivity and specificity.

Originally in the video, I set up the resampled folds like this:

```{r}
set.seed(123)
folds <- vfold_cv(uni_juiced, strata = diversity)
```

But some helpful folks pointed out that this can result in overly optimistic results from resampling (i.e. data leakage) because of some of the recipe steps. It's better to resample the original training data.

```{r}
set.seed(123)
folds <- vfold_cv(uni_train, strata = diversity)
```

After we have these folds, we can use `fit_resamples()` with the recipe to estimate model metrics.

```{r}
set.seed(234)
glm_rs <- glm_spec %>%
  fit_resamples(
    uni_rec,
    folds, 
    metrics = metric_set(roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )

set.seed(234)
knn_rs <- knn_spec %>%
  fit_resamples(
    uni_rec, 
    folds, 
    metrics = metric_set(roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )

set.seed(234)
tree_rs <- tree_spec %>%
  fit_resamples(
    uni_rec, 
    folds, 
    metrics = metric_set(roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
  )
```

What do these results look like?

```{r}
tree_rs
```

We can use `collect_metrics()` to see the summarized performance metrics for each set of resamples.

```{r}
glm_rs %>%
  collect_metrics()
knn_rs %>%
  collect_metrics()
tree_rs %>%
  collect_metrics()
```

In realistic situations, we often care more about one of sensitivity or specificity than overall accuracy.

What does the ROC curve look like for these models?

```{r}
glm_rs %>%
  unnest(.predictions) %>%
  mutate(model = "glm") %>%
  bind_rows(knn_rs %>%
              unnest(.predictions) %>%
              mutate(model = "knn")) %>%
  bind_rows(tree_rs %>%
              unnest(.predictions) %>%
              mutate(model = "rpart")) %>%
  group_by(model) %>%
  roc_curve(diversity, .pred_high)  %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```

If we decide the logistic regression model is the best fit for our purposes, we can look at the parameters in detail.

```{r}
glm_fit %>%
  tidy() %>%
  arrange(-estimate)
```

Larger, less expensive schools in the South and West are more likely to have higher proportions of minority students.

Finally, we can return to our test data as a last, unbiased check on how we can expect this model to perform on new data. We `bake()` our recipe using the testing set to apply the same preprocessing steps that we used on the training data.

```{r}
glm_fit %>% 
  predict(new_data = bake(uni_prep, uni_test), 
          type = "prob") %>%
  mutate(truth = uni_test$diversity) %>%
  roc_auc(truth, .pred_high)
```

We can also explore other metrics with the test set, such as specificity.

```{r}
glm_fit %>% 
  predict(new_data = bake(uni_prep, new_data = uni_test), 
          type = "class") %>%
  mutate(truth = uni_test$diversity) %>% 
  spec(truth, .pred_class)
```

Our metrics for the test set agree pretty well with what we found from resampling, indicating we had a good estimate of how the model will perform on new data.
